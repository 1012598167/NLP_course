{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "import jieba\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Dropout, Conv1D, MaxPooling1D, Bidirectional, Activation\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "i=1\n",
    "word_map={}\n",
    "with open('./fasttext_wordembedding.txt', 'r', encoding='UTF-8') as f:\n",
    "    for line in f:\n",
    "        if i==1:\n",
    "            i=0\n",
    "            continue\n",
    "        Line=re.split(' ',line.strip())\n",
    "        word=Line[0]\n",
    "        x_train=Line[1:]\n",
    "        word_map[word]=x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_map['判决']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_len=233"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=[]\n",
    "to_drop=[]\n",
    "i=0\n",
    "with open('./Label2.txt', 'r', encoding='UTF-8') as f:\n",
    "    for line in f:\n",
    "        list_=[0]* labels_len\n",
    "        Line=re.split(' ',line.strip())\n",
    "        if Line[0]=='0':\n",
    "            to_drop.append(i)\n",
    "        else:\n",
    "            for line in Line:\n",
    "                list_[int(line)-1]=1\n",
    "            Y.append(list_)\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_count=[0]*233\n",
    "for i in Y_choose:#1001\n",
    "    k=0\n",
    "    for j in i:#233\n",
    "        if j==1:\n",
    "            label_count[k]+=1\n",
    "        k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[]#60158篇约500字100维向量\n",
    "j=0\n",
    "max_article=1001#60158\n",
    "not_skip=[]\n",
    "to_skip=[]\n",
    "with open('./Cons.txt', 'r', encoding='UTF-8') as f:\n",
    "    for line in f:#60158\n",
    "        if max_article<0:\n",
    "            break\n",
    "        if j in to_drop:\n",
    "            j+=1\n",
    "            continue\n",
    "        words=line.split(' ')#约500\n",
    "        list_=[]\n",
    "        max_word=100\n",
    "        for word in words:\n",
    "            if word in word_map:\n",
    "                list_.append([float(i) for i in word_map[word]])#100维\n",
    "                max_word-=1\n",
    "                if max_word<=0:\n",
    "                    break\n",
    "        if max_word>0:\n",
    "            to_skip.append(j)\n",
    "            j+=1\n",
    "            continue\n",
    "        not_skip.append(j)\n",
    "        X.append(list_)\n",
    "        max_article-=1\n",
    "        j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consider_index10in233=[161, 196, 49, 45, 130, 49, 205, 204, 8, 155]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_choose=np.array(Y)[not_skip]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key=0#1001有174篇有2个以上  那就挑着174篇 \n",
    "index_label_gt2=[]#174/1001\n",
    "k=0\n",
    "for i in Y_choose.T[consider_index10in233].T:#1001\n",
    "    num=0\n",
    "    for j in i:#233\n",
    "        if j==1:\n",
    "            num+=1\n",
    "        if num==2:\n",
    "            index_label_gt2.append(k)\n",
    "            key+=1\n",
    "            break\n",
    "    k+=1\n",
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_choose.T[consider_index10in233].T[index_label_gt2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "# X Y\n",
    "start = time.time()\n",
    "train_X,test_X,train_y,test_y = train_test_split(np.array(X)[index_label_gt2],Y_choose.T[consider_index10in233].T[index_label_gt2],test_size=0.2,random_state=5)\n",
    "model = Sequential()\n",
    "#model.add(LSTM(input_dim=100, output_dim=50, return_sequences=True))\n",
    "\n",
    "#model.add(LSTM(input_dim=100, units=50, return_sequences=True))#会记录之前信息\n",
    "\n",
    "\n",
    "#model.add(Bidirectional(LSTM(20, return_sequences=True), input_shape=(n_timesteps, 1)))\n",
    "model.add(Bidirectional(LSTM(100, return_sequences=True), input_shape=(50, 1)))#会记录之前信息\n",
    "\n",
    "model.add(Dropout(0.2))#防过拟合提高泛化能力 卷积之后一般不用因为参数少会过拟合，\n",
    "#全连接之后dropout好一点 断掉一些链接 而不是直接断维度\n",
    "model.add(LSTM(30, return_sequences=False))\n",
    "model.add(Dropout(0.2))#不过拟合不加dropout 欠拟合降dropout\n",
    "#dropout的直接作用是减少中间特征的数量，从而减少冗余，即增加每层各个特征之间的正交性\n",
    "# （数据表征的稀疏性观点也恰好支持此解释）。\n",
    "#以概率p对该层神经元进行保留 将输入X转为X/P来训练\n",
    "model.add(Dense(units=10))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "#sgd = optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "adam=optimizers.Adam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy')\n",
    "model.fit(np.array(train_X), np.array(train_y), batch_size=256, epochs=50, validation_split=0.2)# 512 10\n",
    "print(model.summary())#可以看到序列模型的每一层\n",
    "\n",
    "model.save('LSTM1.h5')\n",
    "model.save_weights('LSTMweight.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
